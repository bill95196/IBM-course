{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d019a1e484425ebd2434014a309147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/261482368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb115bfe8b444d769a708d1dc540e046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 03:33:39.163514: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2023-10-09 03:33:39.170529: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394280000 Hz\n",
      "2023-10-09 03:33:39.171224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561ece464580 executing computations on platform Host. Devices:\n",
      "2023-10-09 03:33:39.171277: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2023-10-09 03:33:39.199235: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f7b8c8f0a90>,\n",
       " <keras.layers.core.Dense at 0x7f7b74380a90>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f7c0836cb10>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f7c0268ba90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7c02696790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7c03d94450>,\n",
       " <keras.layers.core.Activation at 0x7f7c03f50c90>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f7c0263b1d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7c03da2d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7c03aa3d10>,\n",
       " <keras.layers.core.Activation at 0x7f7c03aa3c10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7c03aba890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bf874d250>,\n",
       " <keras.layers.core.Activation at 0x7f7bf8700650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bf86cfc90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bf85cc7d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bf86b4b90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bf85df050>,\n",
       " <keras.layers.merge.Add at 0x7f7bf85dff90>,\n",
       " <keras.layers.core.Activation at 0x7f7bf8529d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bf84486d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bf84b6150>,\n",
       " <keras.layers.core.Activation at 0x7f7bf84a1110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bf843c990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bf83038d0>,\n",
       " <keras.layers.core.Activation at 0x7f7bf83ba0d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bf82d3890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bf8271710>,\n",
       " <keras.layers.merge.Add at 0x7f7bf82b5910>,\n",
       " <keras.layers.core.Activation at 0x7f7bf81d4c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bf8173c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bf814a3d0>,\n",
       " <keras.layers.core.Activation at 0x7f7bf814a410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bf80eac90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bf8047a10>,\n",
       " <keras.layers.core.Activation at 0x7f7bf8047e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7be873c810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7be871fd50>,\n",
       " <keras.layers.merge.Add at 0x7f7be871fa10>,\n",
       " <keras.layers.core.Activation at 0x7f7be863cc10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7be8673210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7be85cce90>,\n",
       " <keras.layers.core.Activation at 0x7f7be8636a50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7be854fc10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7be8538f50>,\n",
       " <keras.layers.core.Activation at 0x7f7be8538990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7be84657d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7be8366b50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7be83c6f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7be82e1210>,\n",
       " <keras.layers.merge.Add at 0x7f7be82afa10>,\n",
       " <keras.layers.core.Activation at 0x7f7be8263f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7be818e650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7be81e1c50>,\n",
       " <keras.layers.core.Activation at 0x7f7be81232d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7be8123b50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7be80a0210>,\n",
       " <keras.layers.core.Activation at 0x7f7be80f7790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bc87d48d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bc87b7f90>,\n",
       " <keras.layers.merge.Add at 0x7f7bc87b7e10>,\n",
       " <keras.layers.core.Activation at 0x7f7bc86ecd90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bc868a750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bc864f710>,\n",
       " <keras.layers.core.Activation at 0x7f7bc8669d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bc8590a10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bc8566650>,\n",
       " <keras.layers.core.Activation at 0x7f7bc8566590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bc847fcd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bc8469f90>,\n",
       " <keras.layers.merge.Add at 0x7f7bc83fcc50>,\n",
       " <keras.layers.core.Activation at 0x7f7bc8397cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bc834bb10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bc82fef50>,\n",
       " <keras.layers.core.Activation at 0x7f7bc8294dd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bc823ce50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bc8210310>,\n",
       " <keras.layers.core.Activation at 0x7f7bc83973d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bc81aa390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bc8127b90>,\n",
       " <keras.layers.merge.Add at 0x7f7bc80c1410>,\n",
       " <keras.layers.core.Activation at 0x7f7bc8044150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bc8044e10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bb077dc90>,\n",
       " <keras.layers.core.Activation at 0x7f7bb0744f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bb0708a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bb06d44d0>,\n",
       " <keras.layers.core.Activation at 0x7f7bb06d4790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bb0619750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bb0531d10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bb0580d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bb04ccb90>,\n",
       " <keras.layers.merge.Add at 0x7f7bb04cc890>,\n",
       " <keras.layers.core.Activation at 0x7f7bb0434d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bb03569d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bb03aeb50>,\n",
       " <keras.layers.core.Activation at 0x7f7bb03ae2d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bb02e4e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bb026c290>,\n",
       " <keras.layers.core.Activation at 0x7f7bb0246650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7bb01e0e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bb0127b50>,\n",
       " <keras.layers.merge.Add at 0x7f7bb01598d0>,\n",
       " <keras.layers.core.Activation at 0x7f7bb00f6990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8dfd7710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7bb006e590>,\n",
       " <keras.layers.core.Activation at 0x7f7bb006ee50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8df51150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8df39f90>,\n",
       " <keras.layers.core.Activation at 0x7f7b8decbb50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8de6ad10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8ddf7250>,\n",
       " <keras.layers.merge.Add at 0x7f7b8ddce890>,\n",
       " <keras.layers.core.Activation at 0x7f7b8dd67fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8dd67f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8dce04d0>,\n",
       " <keras.layers.core.Activation at 0x7f7b8dce01d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8dc26a50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8dbf8610>,\n",
       " <keras.layers.core.Activation at 0x7f7b8dbf8fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8db119d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8dafbed0>,\n",
       " <keras.layers.merge.Add at 0x7f7b8dafbd50>,\n",
       " <keras.layers.core.Activation at 0x7f7b8da2ad50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8d9dbb50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8d991fd0>,\n",
       " <keras.layers.core.Activation at 0x7f7b8d9919d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8d8c2710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8d8a4f50>,\n",
       " <keras.layers.core.Activation at 0x7f7b8d7c4950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8d7d7d10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8d73c690>,\n",
       " <keras.layers.merge.Add at 0x7f7b8d73ca50>,\n",
       " <keras.layers.core.Activation at 0x7f7b8d6d3a10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8d6d3d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8d651bd0>,\n",
       " <keras.layers.core.Activation at 0x7f7b8d651a90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8d5e9e10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8d565190>,\n",
       " <keras.layers.core.Activation at 0x7f7b8d565250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8d480a50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8d41ea50>,\n",
       " <keras.layers.merge.Add at 0x7f7b8d3dac90>,\n",
       " <keras.layers.core.Activation at 0x7f7b8d39aa90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8d34e890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8d2fff90>,\n",
       " <keras.layers.core.Activation at 0x7f7b8d2ff4d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8d2b5550>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8d212950>,\n",
       " <keras.layers.core.Activation at 0x7f7b8d212ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8d1b15d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8d069d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8d129150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8cffe050>,\n",
       " <keras.layers.merge.Add at 0x7f7b8cffe390>,\n",
       " <keras.layers.core.Activation at 0x7f7b8cf5d1d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8cea6210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8cec4cd0>,\n",
       " <keras.layers.core.Activation at 0x7f7b8ced8e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8ce787d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8cdeff50>,\n",
       " <keras.layers.core.Activation at 0x7f7b8cd8a250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8cd0ef90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8cc99150>,\n",
       " <keras.layers.merge.Add at 0x7f7b8ccab3d0>,\n",
       " <keras.layers.core.Activation at 0x7f7b8cc07c50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8cc07d10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8cb9f090>,\n",
       " <keras.layers.core.Activation at 0x7f7b8cb64fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8cac8c90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8ca9b390>,\n",
       " <keras.layers.core.Activation at 0x7f7b8ca9b790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f7b8ca3a850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f7b8c9b2f90>,\n",
       " <keras.layers.merge.Add at 0x7f7b8c956bd0>,\n",
       " <keras.layers.core.Activation at 0x7f7b8c8d1dd0>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f7b8cc07f10>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f7c03da2ad0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 03:35:00.171827: W tensorflow/core/framework/allocator.cc:107] Allocation of 321126400 exceeds 10% of system memory.\n",
      "2023-10-09 03:35:05.206729: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2023-10-09 03:35:10.727162: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2023-10-09 03:35:16.806703: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2023-10-09 03:35:22.166355: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15/301 [>.............................] - ETA: 4:23:25 - loss: 0.3456 - acc: 0.8613"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
